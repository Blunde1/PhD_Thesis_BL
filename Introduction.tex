\chapter{Introduction}

Advanced statistical methods and procedures have seen increased and widespread usage in later years.
This is backed by access to more data and new use-cases, cheaper computational power, and adaption into mainstream languages such as Python and R.
Underlying this trend is also the increased usability of said algorithms, in regards to training on data and putting them into production.
The goal of this thesis is to further the usability of computational methods in statistics with regards to stability, speed, and automatic functionality.

The main approach of the present work is to, in some loose and wide sense, approximate some objective function with a local quadratic approximation to either solve stability issues, create dynamic step-lengths, or measure the uncertainty of estimators.
%The technique of such local quadratic approximations in statistics dates back to the usage of the Newton-Raphson algorithm in fitting procedures for maximum-likelihood estimation.
%As for this classical case, 
The hope is then that the iterative methods that the local quadratic approximation is applied to, will see an increased adaptivity to individual data and problems, and corresponding decrease in manual tuning performed before applications to the problem at hand.
%, and corresponding increased adaptivity to individual data and problems.

The first part of this thesis will give a brief and informal introduction to the concepts and techniques that are used in papers I-III.
The basis is the objective of maximum likelihood and supervised learning, which are presented in the first section.
The second section introduces the local quadratic approximation, and showcase it in the relevant use-cases for papers I-III, i.e. maximum-likelihood numerical optimization, the saddlepoint approximation, gradient tree boosting, asymptotic theory and model selection.
The final section summarises the papers of the thesis.
% are presented as the methods under investigation for this thesis, 
%This includes 2'nd order quadratic approximations, and its use-cases in the saddlepoint approximation, gradient boosting and asymptotic theory.
%
%
%In some loose sense, all the included papers in this thesis seek to approximate some objective function with 
%
%The main theme of this thesis is to, in some loose sense, approximate some objective function with a quadratic approximation to either solve stability issues, create dynamic step-length, or measure the uncertainty of estimators.
%Under certain transformations, a quadratic approximation is often highly accurate
%De-facto in the new field of data-science.
%
%
%
%but also the usability
%
%A key
%
%
%Coupled with access to more data and cheaper computational power, advanced statistical methods has seen an increasing use in both business and academia. 
%
%
%This is in no small amount helped by the numerical procedures underlying these methods.
%Adopted into mainstream languages such as Python and R, statistical methods has seen widespread availability, become faster and easier to use.
%and helped by adoption into mainstream languages such as Py and R.
%Easier usage, faster speed, and 
%Dates back to the use of Newton-Raphson to find the maximum-likelihood solutions of logistic-regression.

\chapter{Maximum likelihood and supervised learning}

Maximum likelihood estimation and supervised learning are briefly introduced in an informal manner. %as the fundamental problems motivating the work presented in this thesis.
This is done to provide intuition to the fundamental objectives of the algorithms that are presented, and as motivation to the research on the algorithms's problems presented in this thesis.
% behind our  the underlying objective and goal is for th, and as a basis for further discussions on the problems that fundamentally stem from MLE and SL.

 %stem from these , and where the problems stem from. informally, to provide intuition and as a basis for further discussions.

\section{Maximum likelihood estimation}

Let $\mathbf{x}$ denote an $n$-dimensional vector of observations from a parametric distribution, with density denoted $p(\mathbf{x};\theta_0)$, where $\theta_0\in \Theta,~\Theta\subseteq \mathrm{R}^p$ is a $p$-dimensional vector.
Is often the case that a reasonable parametric family of functions, $p(\mathbf{x};\theta),~\theta\in \Theta$, can be inferred from the problem and from inspection of the data.
However, $\theta_0$ will be unknown, and it is reasonable to estimate it using the observed data $\mathbf{x}$.
To this end, maximum likelihood estimation is a popular approach.
The maximum likelihood estimate (MLE) is the value of $\theta$ in $\Theta$ which maximizes the probability of the data, i.e. the likelihood, 
\begin{align}\label{eq:mle}
	\hat{\theta} = \arg\min_\theta \{-\log p(\mathbf{x};\theta)\}.
\end{align}
The maximum likelihood estimate, $\hat{\theta}$ is, under suitable regularity conditions, the asymptotically unbiased minimum variance estimate, and asymptotically normal.
See -Van der vaart- for a treatment of their asymptotic properties.


\section{Supervised learning}

The supervised learning objective is perhaps easiest stated as "regression", but also bears resemblance to maximum likelihood estimation.
Assume now that $\mathbf{x}\in R^{n\times m}$ is a matrix of $p$ covariates or features for $n$ observations. Let $y\in R^n$ be an $n$-vector of response observations.
In general, individual response observations, $y_i,~i=1\dots n$, could also be multidimensional, but throughout this thesis they are assumed one-dimensional.
Let $\hat{y}_i$ be a prediction for $y_i$ and let the loss function $l(y_i,\hat{y}_i)$ be a function measuring the difference between a response and its prediction.
The supervised learning objective is to find the best possible predictive function, $f(x)=\hat{y}$, which takes a feature vector (row-vector of $\mathbf{x}$) as its argument, and outputs a prediction $\hat{y}$. "Best possible" is here in reference to the loss $l$ over observations not part of the training data $(\mathbf{x},y)$.
More formally, we seek $f$ so that
\begin{align}\label{eq:supervised objective}
	\hat{f} = \arg\min_f \left\{ E\left[l(y^0, f(x^0))\right] \right\},
\end{align} 
where the superscript $(y^0, x^0)$ indicates an observation unseen in the training data, and $E$ denotes the expectation.
Notice that, if the search is constrained over a parametric family of functions indexed by $\theta\in\Theta$, and the loss function is taken to be the negative log-likelihood, $l=-\log p$, then the supervised learning objective is closely related to the objective of maximum likelihood estimation \eqref{eq:mle} in a regression setting.
In fact, the objective in \eqref{eq:mle} is the sample estimator of the expected value in \eqref{eq:supervised objective}, but biased downwards in expectation, as evaluation is done over observations in the training set.
% evaluated over observations not part of the training data.


\chapter{Quadratic approximations in statistics} 

% mention many more related applications: Newton-Raphson, RMHMC, Laplace approx... highly active fields of research
% must be appropriately differentiable

%The title of this thesis is "Information in local curvature: Three papers on adaptive methods in computational statistics".
The maximum likelihood objective \eqref{eq:mle} and supervised learning objective \eqref{eq:supervised objective} are, except for the most trivial of cases, not straightforward, and must be solved numerically.
This then typically involve some iterative algorithm, which may require substantial manual tuning and trial and error before successful application. 
%This is in relation to that 1 and 2 are not always straightforward.
However, a local quadratic approximations to some otherwise intractable function can often be of help in making these algorithms more automatic and adaptive to the data and problem at hand.

When referring to a local quadratic approximation, as is frequently done in this thesis, it is meant to refer to a 2'nd order Taylor approximation of a function $f(x)$, about some point $x_0$.
For example, the quadratic approximation of the negative log-likelihood loss function $l=-\log p$ about some value of $\theta$, say $\theta_k$, gives
\begin{align}\label{eq:quadratic approximation}
	l(y_i,f(x_i;\theta))
	&\approx
	l(y_i,f(x_i;\theta_k))
	+ \nabla_\theta l(y_i,f(x_i;\theta_k)) (\theta-\theta_k)\notag\\
	&+ \frac{1}{2}(\theta-\theta_k)^T \nabla_\theta^2 l(y_i,f(x_i;\theta_k)) (\theta-\theta_k).
\end{align}

\begin{Example}[Newton-Raphson]\label{ex:newton-raphson}
	The MLE $\hat{\theta}$ in \eqref{eq:mle} typically has to be found numerically, as 
	the score equations, $0=\nabla_\theta l(y_i,f(x_i;\theta_k))$, are not possible to solve 
	analytically. Assuming that $l$ is differentiable and convex in $\theta$, the Newton-Raphson algorithm will converge to the MLE $\hat{\theta}$. 
	The iterative Newton-Raphson algorithm is constructed by employing the r.h.s. of \eqref{eq:quadratic approximation} iteratively to the current value of $\theta$, say $\theta_k$, the next value in the iterative algorithm is then given by 
	% VECTOR MATRIX NOTATION -- MUST BE EXPLAINED, PERHAPS BEFORE EQ 2.1?
	\begin{align*}
		\theta_{k+1} = \theta_k 
		- \left[\nabla_\theta^2 l(y_i,f(x_i;\theta_k))\right]^{-1}
		\nabla_\theta l(y_i,f(x_i;\theta_k)),
	\end{align*}
	the MLE if $l$ indeed was equal to the quadratic approximation on the r.h.s. in \eqref{eq:quadratic approximation}.
\end{Example}
%while Fisher-scoring (or, in general, natural gradients) is perhaps preferable due to stability reasons than the NR algorithm, NR and FS is equivalent when the distribution is in the exponential family and canonical parametrization. Which is an important family of functions, implicitly considered in paper II and III.

There are many problems in computational statistics that may be helped by \eqref{eq:quadratic approximation}, however only a few, the ones relevant to papers I-III, are discussed here.
The following sections discuss applications of local quadratic approximations as helpful tools in dealing with some of the problem associate/in dealing numerical optimization of \eqref{eq:mle} and \eqref{eq:supervised objective}.

% NEWTON RAPHSON AS EXAMPLE

%\section{Newton-Raphson}

\section{The saddlepoint approximation}
\begin{figure}[t!]
	\centering
	\includegraphics[width=0.8\textwidth,height=6cm]{Fig/ift_direct_fail.pdf}
	\caption{
	\label{fig:ift-direct-fail}
	Figure included from Paper 1. Illustrating the dominance of inaccuracies of the IFT \eqref{eq:ift}, calculated with quadrature, at machine precision at $\log(1.0\times 10^{-14})$ indicated by the dotted horizontal line.
	}
\end{figure}
It is often the case that the density $p_X(x;\theta)$ of a random variable $X$, is not available in closed form when there are multiple sources of randomness present in $X$.
Direct optimization of \eqref{eq:mle} is therefore difficult.
However, the characteristic function or the Fourier transform of the density, $\varphi_X(s)=E[\exp(isX)]$ often still exhibits closed form, even in situations with more than one source of randomness.
The density might then be retrieved by numerically evaluating the inverse Fourier transform
\begin{align}\label{eq:ift}
	p_X(x;\theta) = \frac{1}{2\pi} \int_{-\infty}^{\infty} \varphi_X(s;\theta)e^{-isx} ds
	= \frac{1}{2\pi} \int_{\infty}^{\infty} e^{K_X(is;\theta)-isx} ds,
\end{align}
where $K_X(s;\theta)=\log\varphi_X(-is;\theta)$ is the cumulative generating function (CGF).

% NUMERICAL PROBLEM
However, consider the case of numerical MLE optimization, for example by using the Newton-Raphson algorithm in Example \ref{ex:newton-raphson}. 
Here, at the first iteration, say $\theta_1$, the initial estimate is likely to start at values far from the population MLE, $\theta_0$.
Necessarily, observations $x$ will take place in low-density regions of $p(x;\theta_1)$, and this will continue to be the case at subsequent iterations, until $\theta_k$ is close to $\theta_0$.
% the log-densities building the likelihood are likely to very small probability mass.
This constitutes a problem to direct numerical inversion of \eqref{eq:ift} using quadrature schemes (weighted sum of integrand evaluations), as numerical inaccuracies related to the (binary) representation of floating-point numbers will dominate.
More specifically, considering double precision at order $1.0\times 10^{-16}$, if $x$ is in a region with log-density $\log p(x;\theta_k)$ smaller than this value, the inaccuracy of the representation is sure to dominate. Even more is that such behaviour/pathologies in practice happens a few orders of magnitude higher than the theoretical limit given above. In Figure \ref{fig:ift-direct-fail}, the error dominates already at $1.0\times 10^{-14}$.
%FOR EXAMPLE

%SPA
An inversion technique that does not suffer from erroneous computations in low-density regions, and in fact is renown for its tail-accuracy, is the saddlepoint approximation (SPA) -cite daniels, Butler-.
It is developed in paper 1 through an argument of exponential tilting, which takes place on the "time-domain" side of the Fourier transform. 
Complimentary, an argument on the "frequency-domain" side is given here, that closely follows the derivation in -citet Butler, chapter-.
First, notice that the value of the integral is unchanged if we integrate through a line parallel to the imaginary axis, say $\tau$,
\begin{align}
	p_X(x;\theta) 
	%&= \frac{1}{2\pi i} \int_{\tau -i\infty}^{\tau + i\infty} e^{K_X(s;\theta)-sx} ds\notag\\
	&= \frac{1}{2\pi} \int_{\infty}^{\infty} e^{K_X(\tau+is;\theta)-(\tau+is)x} ds
\end{align}
Now, apply the quadratic approximation \eqref{eq:quadratic approximation} to the log-integrand,
$K_X(\tau+is;\theta)-(\tau+is)x$,
locally about the value of $\tau$ solving the saddlepoint equations
\begin{align}\label{eq:spa-inner}
	\hat{\tau}=\arg\min_\tau\{ K_X(\tau;\theta)-\tau x\},
\end{align}
henceforth called the saddlepoint.
This then gives the approximation of the log-integrand
\begin{align}
	K_X(\hat{\tau}+is;\theta)-(\hat{\tau}+is)x
	\approx 
	K_X(\hat{\tau})-\hat{\tau}x
	- \frac{1}{2}\frac{d^2}{d\tau^2}K_X(\hat{\tau})s^2.
\end{align}
Inserting this into the integral, and performing the transformation $u=\sqrt{\frac{d^2}{d\tau^2}K_X(\hat{\tau})}s$, then gives the SPA as
\begin{align}\label{eq:spa}
	p_X(x;\theta) 
	&\approx
	\frac{\exp(K_X(\hat{\tau})-\hat{\tau}x)}{2\pi\sqrt{\frac{d^2}{d\tau^2}K_X(\hat{\tau})}}
	\int_{-\infty}^{\infty}e^{-\frac{1}{2}u^2}du \notag \\
	&= \frac{\exp(K_X(\hat{\tau})-\hat{\tau}x)}{\sqrt{2\pi\frac{d^2}{d\tau^2}K_X(\hat{\tau})}}
	= spa_X(x;\theta).
\end{align}

The SPA \eqref{eq:spa} is often accurate, and is asymptotically exact in $n$ if there is some asymptotic normality underlying $X$, for example for $X=n^{-1}\sum_i X_i$.
In particular, its relatively fast computation if implemented using automatic-differentiation software to solve the inner problem \eqref{eq:spa-inner}, and its previously mentioned tail-accuracy, are properties that are highly tractable.
On the down-side, the SPA does not integrate to one, except for in a few special cases.
Also, the approximation is often unimodal even if the target density is multimodal, which could very well be the case when $X$ consists of multiple sources of randomness.
A common technique is to multiply the SPA with a constant value $c$, where $c^{-1}=\int spa_X(x;\theta)dx$, that ensure it is a density.
This is immediately more computationally costly, require bespoke implementation, and does not solve the problems of unimodality. 
These problems are the subject of Paper I.

% LEAD UP TO WHAT IS DONE IN PAPER 1 / MOTIVATE

% and choose the value of $\tau$ as the saddlepoint, $\hat{\tau}=\arg\min_\tau K_X(\tau;\theta)-\tau x$

% TAIL BEHAVIOUR

% accuracy and integration problem



\section{Gradient tree boosting}
\label{sec:gradient tree boosting}
\begin{algorithm*}[h!]
	\begin{tabbing}
		\hspace{2em} \= \hspace{2em} \= \hspace{2em} \= \\
		{\bfseries Input}: \\
		\> - A training set $\{(x_i, y_{i})\}_{i=1}^n$\\
		\> - A differentiable loss function $l(\cdot,\cdot)$\\
		\> - A learning rate $\delta\in(0,1]$\\
		\> - Number of boosting iterations $K$\\
		\> - Type of statistical model $\mathcal{F}$\\
		
		1. Initialize model with a constant value:\\
		\>	$f^{(0)}(x) \equiv \underset{\eta}{\arg\min} \sum_{i=1}^n l(y_i, \eta)$\\
		
		2. {\bfseries for} $k = 1$ to $K$:\\
		
		\>	$i)$ Compute derivatives \eqref{eq:gb-derivatives}\\
		
		\> $ii)$ Fit a statistical model $\tilde{f}\in \mathcal{F}$ to derivatives using \eqref{eq:gb-iteration-loss} \\
		
		\> $iii)$ Scale the model with the learning rate\\
		\>\> $f_k(x)= \delta \tilde{f}(x)$ \\%\sum_{t=1}^{T_k} \hat{w}_{tk}I(q_k(\features)=t)$\\
		
		\>	$iv)$ Update the model:\\
		\>\>	$f^{(k)}(x) = f^{(k-1)}(x) + f_k(x)$\\
		{\bfseries end for} \\
		
		3. Output the model: {\bfseries Return} $f^{(K)}(x)$%=\sum_{k=0}^{K}f_k(\features)$. \\
		
	\end{tabbing}
	
	\vspace{0.5cm}
	If needed explaining text
	\vspace{0.5cm}
	
	\caption{\label{alg:gradient-boosting} Second-order generic gradient boosting \cite{friedman2001elements} }
\end{algorithm*}
\begin{algorithm*}[h!]
	\begin{tabbing}
		\hspace{2em} \= \hspace{2em} \= \hspace{2em} \= \\
		{\bfseries Input}: \\
		\> - A training set with derivatives and features $\{(x_i, g_{i,k}, h_{i,k})\}_{i=1}^n$\\
		
		{\bfseries Do}: \\
		1. Initialize the tree with a constant value $\hat{w}$ in a root node:\\
		\>	$\hat{w} = - \frac{\sum_{i=1}^{n}g_{i,k}}{\sum_{i=1}^{n}h_{i,k}}$\\
		
		2. Choose a leaf node $t$ and let $I_{tk}$ be the index set of observations\\
		\> falling into node $t$\\ 
		\> For each feature $j$, compute the reduction in training loss\\
		\> $ \mathcal{R}_t(j,s_j)=\frac{1}{2n}\left[ \frac{\left(\sum_{i\in I_{L}(j,s:j)}g_{ik}\right)^2}{\sum_{i\in I_{L}(j,s_j)}h_{ik}}
		+ \frac{\left(\sum_{i\in I_{R}(j,s_j)}g_{ik}\right)^2}{\sum_{i\in I_{R}(j,s_j)}h_{ik}}	
		-\frac{\left(\sum_{i\in I_{tk}}g_{ik}\right)^2}{\sum_{i\in I_{tk}}h_{ik}} \right] $\\
		\> 		for different split-points $s_j$, and where\\ 
		\> $I_{L}(j,s_j) = \{ i\in I_{tk}: x_{ij}\leq s_j \}$ and
		 $I_{R}(j, s_j) = \{ i\in I_{tk} : x_{ij} > s_j \}$\\
		\> The values of $j$ and $s_j$ maximizing $\mathcal R_t(j,s_j)$ are chosen as\\
		\> the next split, creating two new leaves from the old leaf $t$.\\
		
		3. Continue step 2 iteratively, until some threshold on\\
		\> tree-complexity is reached.
		
	\end{tabbing}
	
	\vspace{0.5cm}
	If needed explaining text
	\vspace{0.5cm}
	
	\caption{\label{alg:recursive-binary-splitting} Greedy recursive binary splitting, from Paper II }
\end{algorithm*}

The idea behind gradient boosting emerged in \citet{friedman2001greedy} and in particular in \citet{mason1999boosting} as a way to approximate functional gradient descent for the optimization problem in \eqref{eq:supervised objective}, similarly to how the Newton-Raphson algorithm from Example \ref{ex:newton-raphson} solves the optimization problem in \eqref{eq:mle}:
Given an initial function $f^0(x)=f_0(x)$, one ideally seeks a function $f_1(x)$ minimizing
\begin{align}
	\hat{f}_1(x) = \arg\min_{f_1} E\left[ l\left(y,f^0(x)+f_1(x)\right) \right].
\end{align}
If this is difficult, a reasonable substitute to $\hat{f}_1$ is to find the functional derivative of this objective and add the negative direction to the model, say $f^1 = f_0 + f_1$, and then repeat the procedure until convergence at iteration $K$, which would yield the final model $\hat{f}=f^0+\dots+f^K$.

Difficulties arise to this procedure, as the joint distribution of $(y,x)$ is generally unknown. Therefore, the expectation cannot be computed explicitly and neither can the functional derivative.
The immediate solution, if having access to a dataset $\mathcal{D}_n=\{y_i,x_i\}_{i=1}^n$ of independent observations, is to average the loss over these observations, and instead, at iteration $k$, seek
\begin{align}
	\hat{f}_k(x) = \arg\min_{f_k} \frac{1}{n}\sum_{i=1}^{n} l\left( y_i, f^{k-1}(x_i)+f_k(x_i) \right).
\end{align}
% COMMENT ON OVERFITTING
% COMMENT ON FUNCTION OPTIMIZATION, NEEDS TO BE CONSTRAINED
% DEFINE DERIVATIVES
To learn a function that is as close as possible (given the information in the sample) to the functional derivative, use the predictions resulting from the current model, $f^{k-1}$, to compute predictions $\hat{y}_i^{(k-1)}=f_0(x_i)+\dots+f^{(k-1)}(x_i)$, and then derivatives for observations in the sample as 
\begin{align}\label{eq:gb-derivatives}
g_{i,k} = \frac{\partial}{\partial \hat{y_i}}l(y_i,\hat{y}_i^{(k-1)})
,~
h_{i,k} = \frac{\partial^2}{\partial \hat{y_i}^2}l(y_i,\hat{y}_i^{(k-1)}).
%,~
%\hat{y}^{(k-1)} = f^{(k-1)}(x),
\end{align}
These are used in a 2'nd order approximation to the original loss
\begin{align}\label{eq:gb-iteration-loss}
	\hat{f}_k(x) &= \arg\min_{f_k}
	\frac{1}{n}\sum_{i=1}^{n}l(y_i,\hat{y}_i^{(k-1)}) + g_{i,k}f_k(x_i) + \frac{1}{2}h_{i,k}f_k(x_i)^2
	\notag \\
	&= \arg\min_{f_k} \frac{1}{n}\sum_{i=1}^{n} g_{i,k}f_k(x_i) + \frac{1}{2}h_{i,k}f_k(x_i)^2
\end{align}
which is quadratic-type loss amenable to fast optimization.
Now, a function that completely minimizes the above sample loss (both the original and/or the approximate), is likely to adapt to the inherent randomness in the sample.
Furthermore, a search over all possible functions is obviously infeasible.
For these reasons, the search is constrained to a family of functions that admits fast fitting routines/optimization, and that are somehow constrained and thus less likely to overfit.
Typical families include linear functions, local regression using kernels, or most popularly, trees.
%CHECK IF TRAINING LOSS IS DEFINED

%Necessarily, the expectation cannot be computed explicitly and neither can the functional derivative.

Gradient boosting emerges as the collection of the above-mentioned ideas: Iteratively, compute derivative information or pseudo-residuals through \eqref{eq:gb-derivatives}, and fit a statistical model $\tilde{f}_k$ to these observations using \eqref{eq:gb-iteration-loss}.
The final ingredient of gradient boosting is to shrink the model by some constant $\delta\in(0,1]$, $\hat{f}_k=\delta \tilde{f}_k$ to make space for new models, and add it to the ensemble model $f^{(k)}=f^{(k-1)}+\hat{f}_k$.
The gradient boosting pseudo-algorithm is given in Algorithm \ref{alg:gradient-boosting}.
Note that this is the modern-type gradient boosting algorithm, slightly different from the original algorithm of \citet{friedman2001greedy} which is a first-order type algorithm, that would fit the model using mean-squared-error loss, then scale the model with an optimized constant value and finally shrink it.


% LINEAR FUNCTIONS (SEQUENTIALY) --> SPARSE MODELS, LASSO RELATION

% WE WANT TO KEEP SPARSITY, BUT GAIN NON-LINEARITY AND INTERACTION EFFECTS --> TREES
The choice of statistical model to fit to the pseudo-residuals is not arbitrary.
The popular choice is to use classification and regression trees (CART) \citep{breiman1984classification}, which gives gradient tree boosting, the boosting type that has dominated in many machine-learning competitions since the introduction of xgboost \citep{chen2016xgboost}.
Using CART as week learners can be motivated by first considering linear functions, $\hat{y}=\beta^Tx$, where only a portion of the full estimate of the $\hat{\beta}_j$ decreasing \eqref{eq:gb-iteration-loss} the most is used. This then resembles a type of shrinked foreward stagewise procedure, which is closely related to computing LASSO solution paths \citep{friedman2001elements}.
Boosting thus holds the possibility of efficiently building sparse models in the face of high-dimensional problems, by excluding features $x_j$ that does not contribute significant decreases in \eqref{eq:gb-iteration-loss}.
If a week learner is used that fits and use all features simultaneously, this pathology of boosting is likely to disappear.

If CART is fit using greedy binary splitting (Algorithm \ref{alg:recursive-binary-splitting}), then features are used sequentially in the learning procedure.
%CART uses features sequentially if trained with greedy binary splitting (Algorithm XX), which is the popular choice.
Furthermore, in contrast to linear functions, CART can learn non-linear functions and interaction effects automatically.
In essence, gradient tree boosting may learn sparse, non-linear models with complex interaction effects efficiently, while the shrinkage $\delta$ applied to each tree will smooth out the piecewise constant functions.
Model complexity may range from the constant model, to high-dimensional non-linear functions, and in between is often something that may decrease the objective in \eqref{eq:supervised objective} more than other types of (fixed complexity) models.

% TUNING
Gradient boosting originally has two hyperparameters, namely the number of boosting iterations $K$, and shrinkage or the learning rate $\delta$, which is usually set to some "small" value.
Using CART as week learners introduces additional tuning to control the complexity of individual trees.
\citet{friedman2000additive} suggests global hyperparameters fixed equally for all trees, as the greedy binary splitting algorithm is optimized as if the current boosting iteration is the last iteration.
Important such hyperparameters are a parameter for the maximum depth of trees, a maximum number of leaves or terminal nodes in a tree, and a threshold for minimum reduction in training loss \eqref{eq:gb-iteration-loss} if a split is to be performed.
Hyperparamters are typically learned using $k$-fold cross-validation (CV) \citep{stone1974cross}, which increase computation times significantly.



% IN THE END: COMMENT ON L1-L2, AND SAMPLING


%The search among all possible functions have been constrained to some statistical model.
%
%
% might hope to find the functional derivative of the objective in \eqref{eq:supervised objective} with respect to $f$, say $-f^1$, and add the negative direction to the initial function, obtaining $f^0+f^1$,
%
% and then repeat the procedure until convergence at iteration $K$ which yields the final model $\hat{f}^K=f_0+\dots+f_K$.
%However, as the joint distribution of $(y,x)$ is generally unknown, the expectation cannot be computed explicitly and neither can the functional derivative.
%Gradient boosting solves this by iteratively computing derivatives from the objective, and fitting a statistical model to this derivative information.
%
%Viewing gradient boosting as functional gradient descent is correct in the sense that each $f_k$ approximates some functional gradient.
%
%First, the expectation is approximated by averaging, and the objective is approximated by a Taylor expansion
%
%
%% WRITE OUT THE ALGORITHM
%
%
%
%
%
%This then minimizes an approximation to the original objective.


\section{First order asymptotics}
\label{sec:first order asymptotics}


\section{Model selection}
\label{sec:model selection}

% INTRODUCE GEN LOSS --> TIC/NIC
% mention BIC, also resulting from quadratic (laplace) approximation? related to spa



\chapter{Summary of the papers}
% PAPER I

The first paper of the thesis, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Fusce iaculis nulla in est pellentesque ultrices. Nulla ex eros, iaculis vel malesuada vitae, iaculis eu mauris. Nullam sed bibendum erat. Cras tristique augue quis risus egestas porta. Aenean gravida quam congue, malesuada massa nec, vestibulum neque. Suspendisse potenti. Pellentesque non enim efficitur, lobortis ex ac, pulvinar sem. Cras sed velit tincidunt, pharetra elit a, pretium nunc. Donec imperdiet sem a nunc porttitor, non elementum ipsum blandit. Vivamus imperdiet ipsum quis justo gravida blandit. Vivamus sed mauris sed urna maximus pretium. Donec luctus placerat turpis. Praesent dui augue, viverra ultrices lacinia ac, efficitur quis nisi. Nullam pulvinar orci eu lacus mollis, molestie placerat tortor posuere.


% PAPER II
In the second paper, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Fusce iaculis nulla in est pellentesque ultrices. Nulla ex eros, iaculis vel malesuada vitae, iaculis eu mauris. Nullam sed bibendum erat. Cras tristique augue quis risus egestas porta. Aenean gravida quam congue, malesuada massa nec, vestibulum neque. Suspendisse potenti. Pellentesque non enim efficitur, lobortis ex ac, pulvinar sem. Cras sed velit tincidunt, pharetra elit a, pretium nunc. Donec imperdiet sem a nunc porttitor, non elementum ipsum blandit. Vivamus imperdiet ipsum quis justo gravida blandit. Vivamus sed mauris sed urna maximus pretium. Donec luctus placerat turpis. Praesent dui augue, viverra ultrices lacinia ac, efficitur quis nisi. Nullam pulvinar orci eu lacus mollis, molestie placerat tortor posuere.


% PAPER III
The third paper, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Fusce iaculis nulla in est pellentesque ultrices. Nulla ex eros, iaculis vel malesuada vitae, iaculis eu mauris. Nullam sed bibendum erat. Cras tristique augue quis risus egestas porta. Aenean gravida quam congue, malesuada massa nec, vestibulum neque. Suspendisse potenti. Pellentesque non enim efficitur, lobortis ex ac, pulvinar sem. Cras sed velit tincidunt, pharetra elit a, pretium nunc. Donec imperdiet sem a nunc porttitor, non elementum ipsum blandit. Vivamus imperdiet ipsum quis justo gravida blandit. Vivamus sed mauris sed urna maximus pretium. Donec luctus placerat turpis. Praesent dui augue, viverra ultrices lacinia ac, efficitur quis nisi. Nullam pulvinar orci eu lacus mollis, molestie placerat tortor posuere.
