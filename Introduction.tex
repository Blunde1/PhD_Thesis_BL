\chapter{Introduction}

Advanced statistical methods and procedures have seen increased and widespread usage in later years.
This is backed by access to more data and new use-cases, cheaper computational power, and adoption into mainstream languages such as Python and R.
Underlying this trend is also the increased usability of said algorithms, in regards to training on data and putting them into production.
The goal of this thesis is to further the usability of computational methods in statistics with regards to stability, speed, and automatic functionality.

The main approach of the present work is to, in some loose and wide sense, approximate some objective function with a local quadratic approximation to either solve stability issues, create dynamic step-lengths, or measure the uncertainty of estimators.
%The technique of such local quadratic approximations in statistics dates back to the usage of the Newton-Raphson algorithm in fitting procedures for maximum-likelihood estimation.
%As for this classical case, 
The hope is that the iterative methods that the local quadratic approximation is somehow applied to, will see an increased adaptivity to individual data and problems and correspondingly decrease the manual tuning performed.
%, and corresponding decrease in manual tuning performed before application to the problem at hand.
%, and corresponding increased adaptivity to individual data and problems.

The first part of this thesis will give a brief and informal introduction to the concepts and techniques that are used in papers \Romannum{1}-\Romannum{3}.
The basis is the objectives of maximum likelihood and supervised learning, which are presented in the Section \ref{chap:ml-and-sl}.
Section \ref{chap:quadratics} introduces the local quadratic approximation and showcase it in the relevant use-cases for papers \Romannum{1}-\Romannum{3}, i.e. maximum likelihood numerical optimization, the saddlepoint approximation, gradient tree boosting, some asymptotic theory and model selection.
Section \ref{chap:computation} gives a summary of the important computational frameworks for this thesis, while Section \ref{chap:summary} summarises the papers of the thesis.
% are presented as the methods under investigation for this thesis, 
%This includes 2'nd order quadratic approximations, and its use-cases in the saddlepoint approximation, gradient boosting and asymptotic theory.
%
%
%In some loose sense, all the included papers in this thesis seek to approximate some objective function with 
%
%The main theme of this thesis is to, in some loose sense, approximate some objective function with a quadratic approximation to either solve stability issues, create dynamic step-length, or measure the uncertainty of estimators.
%Under certain transformations, a quadratic approximation is often highly accurate
%De-facto in the new field of data-science.
%
%
%
%but also the usability
%
%A key
%
%
%Coupled with access to more data and cheaper computational power, advanced statistical methods has seen an increasing use in both business and academia. 
%
%
%This is in no small amount helped by the numerical procedures underlying these methods.
%Adopted into mainstream languages such as Python and R, statistical methods has seen widespread availability, become faster and easier to use.
%and helped by adoption into mainstream languages such as Py and R.
%Easier usage, faster speed, and 
%Dates back to the use of Newton-Raphson to find the maximum-likelihood solutions of logistic-regression.







