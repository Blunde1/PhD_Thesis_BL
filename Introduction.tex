\chapter{Introduction}

Advanced statistical methods and procedures have seen increased and widespread usage in later years.
This is backed by access to more data and new use-cases, cheaper computational power, and adaption into mainstream languages such as Python and R.
Underlying this trend is also the increased usability of said algorithms, in regards to training on data and putting them into production.
The goal of this thesis is to further the usability of computational methods in statistics with regards to stability, speed, and automatic functionality.

The main approach of the present work is to, in some loose and wide sense, approximate some objective function with a local quadratic approximation to either solve stability issues, create dynamic step-lengths, or measure the uncertainty of estimators.
%The technique of such local quadratic approximations in statistics dates back to the usage of the Newton-Raphson algorithm in fitting procedures for maximum-likelihood estimation.
%As for this classical case, 
The hope is then that the iterative methods that the local quadratic approximation is applied to, will see an increased adaptivity to individual data and problems, and corresponding decrease in manual tuning performed before applications to the problem at hand.
%, and corresponding increased adaptivity to individual data and problems.

The first part of this thesis will give a brief and informal introduction to the concepts and techniques that are used in papers I-III.
The basis is the objective of maximum likelihood and supervised learning, which are presented in the first section.
The second section introduces the local quadratic approximation, and showcase it in the relevant use-cases for papers I-III, i.e. maximum-likelihood numerical optimization, the saddlepoint approximation, gradient tree boosting, and asymptotic theory.
The final section summarises the papers of the thesis.
% are presented as the methods under investigation for this thesis, 
%This includes 2'nd order quadratic approximations, and its use-cases in the saddlepoint approximation, gradient boosting and asymptotic theory.
%
%
%In some loose sense, all the included papers in this thesis seek to approximate some objective function with 
%
%The main theme of this thesis is to, in some loose sense, approximate some objective function with a quadratic approximation to either solve stability issues, create dynamic step-length, or measure the uncertainty of estimators.
%Under certain transformations, a quadratic approximation is often highly accurate
%De-facto in the new field of data-science.
%
%
%
%but also the usability
%
%A key
%
%
%Coupled with access to more data and cheaper computational power, advanced statistical methods has seen an increasing use in both business and academia. 
%
%
%This is in no small amount helped by the numerical procedures underlying these methods.
%Adopted into mainstream languages such as Python and R, statistical methods has seen widespread availability, become faster and easier to use.
%and helped by adoption into mainstream languages such as Py and R.
%Easier usage, faster speed, and 
%Dates back to the use of Newton-Raphson to find the maximum-likelihood solutions of logistic-regression.

\chapter{Maximum likelihood and supervised learning}

Maximum likelihood estimation and supervised learning are briefly introduced in an informal manner. %as the fundamental problems motivating the work presented in this thesis.
This is done to provide intuition to the fundamental objectives of the algorithms that are presented, and as motivation to the research on the algorithms's problems presented in this thesis.
% behind our  the underlying objective and goal is for th, and as a basis for further discussions on the problems that fundamentally stem from MLE and SL.

 %stem from these , and where the problems stem from. informally, to provide intuition and as a basis for further discussions.

\section{Maximum likelihood estimation}

Let $\mathbf{x}$ denote an $n$-dimensional vector of observations from a parametric distribution, with density denoted $p(\mathbf{x};\theta_0)$, where $\theta_0\in \Theta,~\Theta\subseteq \mathrm{R}^p$ is a $p$-dimensional vector.
Is often the case that a reasonable parametric family of functions, $p(\mathbf{x};\theta),~\theta\in \Theta$, can be inferred from the problem and from inspection of the data.
However, $\theta_0$ will be unknown, and it is reasonable to estimate it using the observed data $\mathbf{x}$.
To this end, maximum likelihood estimation is a popular approach.
The maximum likelihood estimate (MLE) is the value of $\theta$ in $\Theta$ which maximizes the probability of the data, i.e. the likelihood, 
\begin{align}\label{eq:mle}
	\hat{\theta} = \arg\min_\theta \{-\log p(\mathbf{x};\theta)\}.
\end{align}
The maximum likelihood estimate, $\hat{\theta}$ is, under suitable regularity conditions, the asymptotically unbiased minimum variance estimate, and asymptotically normal.
See -Van der vaart- for a treatment of their asymptotic properties.


\section{Supervised learning}

The supervised learning objective is perhaps easiest stated as "regression", but also bears resemblance to maximum likelihood estimation.
Assume now that $\mathbf{x}\in R^{n\times m}$ is a matrix of $p$ covariates or features for $n$ observations. Let $y\in R^n$ be an $n$-vector of response observations.
In general, individual response observations, $y_i,~i=1\dots n$, could also be multidimensional, but throughout this thesis they are assumed one-dimensional.
Let $\hat{y}_i$ be a prediction for $y_i$ and let the loss function $l(y_i,\hat{y}_i)$ be a function measuring the difference between a response and its prediction.
The supervised learning objective is to find the best possible predictive function, $f(x)=\hat{y}$, which takes a feature vector (row-vector of $\mathbf{x}$) as its argument, and outputs a prediction $\hat{y}$. "Best possible" is here in reference to the loss $l$ over observations not part of the training data $(\mathbf{x},y)$.
More formally, we seek $f$ so that
\begin{align}\label{eq:supervised objective}
	\hat{f} = \arg\min_f \left\{ E\left[l(y^0, f(x^0))\right] \right\},
\end{align} 
where the superscript $(y^0, x^0)$ indicates an observation unseen in the training data, and $E$ denotes the expectation.
Notice that, if the search is constrained over a parametric family of functions indexed by $\theta\in\Theta$, and the loss function is taken to be the negative log-likelihood, $l=-\log p$, then the supervised learning objective is closely related to the objective of maximum likelihood estimation \eqref{eq:mle} in a regression setting.
In fact, the objective in \eqref{eq:mle} is the sample estimator of the expected value in \eqref{eq:supervised objective}, but biased downwards in expectation, as evaluation is done over observations in the training set.
% evaluated over observations not part of the training data.


\chapter{Quadratic approximations in statistics} 

% mention many more related applications: Newton-Raphson, RMHMC, Laplace approx... highly active fields of research
% must be appropriately differentiable

%The title of this thesis is "Information in local curvature: Three papers on adaptive methods in computational statistics".
The maximum likelihood objective \eqref{eq:mle} and supervised learning objective \eqref{eq:supervised objective} are, except for the most trivial of cases, not straightforward, and must be solved numerically.
This then typically involve some iterative algorithm, which may require substantial manual tuning and trial and error before successful application. 
%This is in relation to that 1 and 2 are not always straightforward.
However, a local quadratic approximations to some otherwise intractable function can often be of help in making these algorithms more automatic and adaptive to the data and problem at hand.

When referring to a local quadratic approximation, as is frequently done in this thesis, it is meant to refer to a 2'nd order Taylor approximation of a function $f(x)$, about some point $x_0$.
For example, the quadratic approximation of the negative log-likelihood loss function $l=-\log p$ about some value of $\theta$, say $\theta_k$, gives
\begin{align}\label{eq:quadratic approximation}
	l(y_i,f(x_i;\theta))
	&\approx
	l(y_i,f(x_i;\theta_k))
	+ \nabla_\theta l(y_i,f(x_i;\theta_k)) (\theta-\theta_k)\notag\\
	&+ \frac{1}{2}(\theta-\theta_k)^T \nabla_\theta^2 l(y_i,f(x_i;\theta_k)) (\theta-\theta_k).
\end{align}

\begin{Example}[Newton-Raphson]
	The MLE $\hat{\theta}$ in \eqref{eq:mle} typically has to be found numerically, as 
	the score equations, $0=\nabla_\theta l(y_i,f(x_i;\theta_k))$, are not possible to solve 
	analytically. Assuming that $l$ is differentiable and convex in $\theta$, the Newton-Raphson algorithm will converge to the MLE $\hat{\theta}$. 
	The iterative Newton-Raphson algorithm is constructed by employing the r.h.s. of \eqref{eq:quadratic approximation} iteratively to the current value of $\theta$, say $\theta_k$, the next value in the iterative algorithm is then given by 
	\begin{align*}
		\theta_{k+1} = \theta_k 
		- \left[\nabla_\theta^2 l(y_i,f(x_i;\theta_k))\right]^{-1}
		\nabla_\theta l(y_i,f(x_i;\theta_k)),
	\end{align*}
	the MLE if $l$ indeed was equal to the quadratic approximation on the r.h.s. in \eqref{eq:quadratic approximation}.
\end{Example}
%while Fisher-scoring (or, in general, natural gradients) is perhaps preferable due to stability reasons than the NR algorithm, NR and FS is equivalent when the distribution is in the exponential family and canonical parametrization. Which is an important family of functions, implicitly considered in paper II and III.

There are many problems in computational statistics that may be helped by \eqref{eq:quadratic approximation}, however only a few, the ones relevant to papers I-III, are discussed here.
The following sections discuss applications of local quadratic approximations as helpful tools in dealing with some of the problem associate/in dealing numerical optimization of \eqref{eq:mle} and \eqref{eq:supervised objective}.

% NEWTON RAPHSON AS EXAMPLE

%\section{Newton-Raphson}

\section{The saddlepoint approximation}

It is often the case that the density $p_X(x)$ of a random variable $X$, is not available in closed form when there are multiple sources of randomness present in $X$.
Direct optimization of \eqref{eq:mle} is therefore difficult.
However, the characteristic function or the Fourier transform of the density, $\varphi_X(s)=E[\exp(isX)]$ often still exhibits closed form, even in situations with more than one source of randomness.
The density might then be retrieved by numerically evaluating the inverse Fourier transform
\begin{align}\label{eq:ift}
	p_X(x;\theta) = \frac{1}{2\pi} \int_{-\infty}^{\infty} \varphi_X(s;\theta)e^{-isx} ds
	= \frac{1}{2\pi i} \int_{-i\infty}^{i\infty} e^{K_X(s;\theta)-sx} ds,
\end{align}
where $K_X(s;\theta)=\log\varphi_X(-is;\theta)$ is the cumulative generating function (CGF).
% NUMERICAL PROBLEM

%SPA
First, notice that the value of the integral is unchanged if we integrate through a line parallel to the imaginary axis, say $\tau$,
\begin{align}
	p_X(x;\theta) 
	= \frac{1}{2\pi i} \int_{-i\infty+\tau}^{i\infty+tau} e^{K_X(s;\theta)-sx} ds
	= \frac{1}{2\pi} \int_{-i\infty}^{i\infty} e^{K_X(\tau+is;\theta)-(\tau+is)x} ds
\end{align}
Now, apply the quadratic approximation \eqref{eq:quadratic approximation} to the log-integrand, and choose the value of $\tau$ as the saddlepoint, $\hat{\tau}=\arg\min_\tau K_X(\tau;\theta)-\tau x$

% TAIL BEHAVIOUR

% accuracy and integration problem



\section{Gradient tree boosting}

\section{First order asymptotics}



\chapter{Summary of the papers}
% PAPER I

The first paper of the thesis, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Fusce iaculis nulla in est pellentesque ultrices. Nulla ex eros, iaculis vel malesuada vitae, iaculis eu mauris. Nullam sed bibendum erat. Cras tristique augue quis risus egestas porta. Aenean gravida quam congue, malesuada massa nec, vestibulum neque. Suspendisse potenti. Pellentesque non enim efficitur, lobortis ex ac, pulvinar sem. Cras sed velit tincidunt, pharetra elit a, pretium nunc. Donec imperdiet sem a nunc porttitor, non elementum ipsum blandit. Vivamus imperdiet ipsum quis justo gravida blandit. Vivamus sed mauris sed urna maximus pretium. Donec luctus placerat turpis. Praesent dui augue, viverra ultrices lacinia ac, efficitur quis nisi. Nullam pulvinar orci eu lacus mollis, molestie placerat tortor posuere.


% PAPER II
In the second paper, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Fusce iaculis nulla in est pellentesque ultrices. Nulla ex eros, iaculis vel malesuada vitae, iaculis eu mauris. Nullam sed bibendum erat. Cras tristique augue quis risus egestas porta. Aenean gravida quam congue, malesuada massa nec, vestibulum neque. Suspendisse potenti. Pellentesque non enim efficitur, lobortis ex ac, pulvinar sem. Cras sed velit tincidunt, pharetra elit a, pretium nunc. Donec imperdiet sem a nunc porttitor, non elementum ipsum blandit. Vivamus imperdiet ipsum quis justo gravida blandit. Vivamus sed mauris sed urna maximus pretium. Donec luctus placerat turpis. Praesent dui augue, viverra ultrices lacinia ac, efficitur quis nisi. Nullam pulvinar orci eu lacus mollis, molestie placerat tortor posuere.


% PAPER III
The third paper, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Fusce iaculis nulla in est pellentesque ultrices. Nulla ex eros, iaculis vel malesuada vitae, iaculis eu mauris. Nullam sed bibendum erat. Cras tristique augue quis risus egestas porta. Aenean gravida quam congue, malesuada massa nec, vestibulum neque. Suspendisse potenti. Pellentesque non enim efficitur, lobortis ex ac, pulvinar sem. Cras sed velit tincidunt, pharetra elit a, pretium nunc. Donec imperdiet sem a nunc porttitor, non elementum ipsum blandit. Vivamus imperdiet ipsum quis justo gravida blandit. Vivamus sed mauris sed urna maximus pretium. Donec luctus placerat turpis. Praesent dui augue, viverra ultrices lacinia ac, efficitur quis nisi. Nullam pulvinar orci eu lacus mollis, molestie placerat tortor posuere.
